<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link rel="stylesheet" href="./assets/css/style.css">
</head>

<body>

    <body>
        <header>
            <a href="#" class="logo">CS1102 Group Project</a>
            <nav>
                <ul>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="Group Project-Introduction to AI image generation.html" class="active">Introduction</a></li>
                    <li><a href="Group Project- Personal Experience with AI Image Generation .html">Personal Experience with AI Image Generation</a></li>
                    <li><a href="Group Project Application and benefits .html">03-Pros & Cons</a></li>
                    <li><a href="Group Project-Ethical Considerations and Future Prospects.html">Ethical Considerations and Future Prospect</a></li>
                </ul>
            </nav>

        </header>
        <canvas id="matrixRain"></canvas>
        <script src="assets\JS\MatrixEffect.js"></script>
        <section class="container">
            <h1 class="SectionHeader">Architecture of LLMs</h1>
            <img src="assets\img\LLMs-Building-Blocks.jpg" alt="">
            <main>
                <section class="intro">
                    <h2>Personal Experience with AI Image Generation</h2>
                    <p> I used Copilot as one of the AI image generating tools. It is very strong oon generating images. I was quite 
                        impressed with its capability. 
                        I ask Copilot to generate an image which shows family element. It gave me a sweet family image, which sticks to my   
                        request.
                    </p>
                    <img src="Copilot gen family1.jpg">
                    <h2>Some small limitation on using Copilot image generation</h2>
                    <ol>
                        <li>blue: attention between prefix tokens</li>
                        <li>green: attention between prefix and target tokens</li>
                        <li>yellow: attention between target tokens, grey: masked attention</li>
                    </ol>
                    <img src="Copilot gen family2.jpg">
                    <h3>Causal Decoder Architecture</h3>
                    <p> This architecture includes a one-way attention mask to ensure that
                        each input token can focus on the past tokens and itself only. The input and output tokens are
                        processed in the same way by the decoder.</p>
                    <h3>Prefix Decoder Architecture:</h3>
                    <p> This architecture modifies the mask mechanism of the causal decoder
                        to allow two-way attention to the prefix token and only one-way attention to the generated
                        token. In this way, just like encoder-decoder architecture, the prefix decoder can encode the
                        prefix sequence bidirectionally and automatically regression to predict the output label one by
                        one, where the same parameters are shared during encoding and decoding. Instead of pre-training
                        from scratch, it seems better to continuously train the causal decoder and then convert it into
                        a prefix decoder to speed up the convergence.</p>
                    <h3>Encoder-decoder Architecture:</h3>
                    <p> This architecture consists of two stacks of Transformer modules, one
                        as an encoder and other one as a decoder. The encoder encodes the input sequence with stacked
                        multi-head self-attention layers to generate its underlying representation, while the decoder
                        cross-focuses on these representations and autoregressively generates the target sequence.</p>

                </section>
            </main>
            <main>
                <section class="intro">
                    <h2>Emergent Abilities of LLMs:</h2>
                    <p>These are the main differences between large language models and small language models that
                        including the following abilities:</p>
                    <ul>
                        <li>In-context learning: If natural language instructions and multiple task demonstrations has
                            been already provided by the language model, the LLMs can generate the expected output for
                            the test instance by completing a sequence of words from the input text without additional
                            training or gradient updates.</li>
                        <li>Instruction following: By fine-tuning multitask datasets formatted with natural language
                            descriptions, LLM performs well on invisible tasks described in the form of instructions.
                            With instruction tuning, LLM can follow the task instructions of a new task without using
                            explicit examples, which shown a higher generalization ability.</li>
                        <li>Step-by-step reasoning: By cue strategy of Chain of Thought (CoT), LLMS can solve complex
                            tasks involving multiple reasoning steps that small language model cannot do by utilizing a
                            cue mechanism involving intermediate reasoning steps to derive a final answer. Presumably,
                            this ability may have been acquired through code training.</li>
                    </ul>

                </section>
            </main>
            <main>
                <section class="intro">
                    <h2>Key Techniques for LLMs:</h2>
                    <ul>
                        <li>Scaling: Scaling laws are formed to improve the capacity of large language models by help to
                            predict the task levels and the performance of larger models based on that of smaller
                            models.</li>
                        <li>Training: To give a more accurate response to the tasks, numerous of data sets and
                            information are input to train the models.</li>
                        <li>Ability eliciting: After pre-training in large-scale corpora, LLMs are endowed with the
                            potential ability as a general task solver.</li>
                        <li>Alignment tuning: To align the response given by LLMs with human values to avoid any content
                            that is toxic, biased, or even harmful to human.</li>
                        <li>Tools manipulation: To compensate the insufficiency of LLMs in tasks other than text
                            generators, LLMs are designed to have the ability to manipulate external tools like
                            calculators or search engines â€¦etc.</li>
                    </ul>
                </section>
            </main>


        </section>
        <section class="watermarks">
               <h4>CS1102 - Course Project - 2024-2025 Semester A WED 17:00-17:50 Project Group 4</h4>
            <p>Ng Yin Cheung - 57845100 Shu Yat San - 57898873 Wan Pui Yee - 57850515 Tong Wai Hin - 58062705</p>
        </section>
    </body>

</html>
